'''
File: train_model.py
Author: moye12325
Description: 1:æ¨¡å‹è®­ç»ƒ
Created: $TIME
Version: v1.0

ä¿®æ”¹è®°å½•:
Date        Author        Modification Content
2025/2/19   moye12325     æ·»åŠ æ–‡ä»¶æ³¨é‡Š
'''
from datetime import datetime
import re
import os
from config import Config
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import transforms
from my_dataset import ImageSegmentationDataset, joint_transforms, joint_transforms_albu  # æ–°å¢ joint_transforms_albu
from NestedUNet import NestedUNet  # æ¨¡å‹å®šä¹‰æ–‡ä»¶
from sklearn.model_selection import train_test_split
from functools import partial

# ======================= 1. è®¾å¤‡é…ç½® =======================
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)

# è¶…å‚æ•°
params = {
    "batch_size": Config.TRAIN_PARAMS['batch_size'],
    "learning_rate": Config.TRAIN_PARAMS['learning_rate'],
    "num_epochs": Config.TRAIN_PARAMS['num_epochs'],
    "num_classes": Config.TRAIN_PARAMS['num_classes'],
    "patience": Config.TRAIN_PARAMS['patience'],
    "weight_decay": Config.TRAIN_PARAMS['weight_decay'],
    "image_size": Config.IMAGE_SIZE,
    "model_version": Config.TRAIN_PARAMS['model_version'],  # ğŸ”´ æ‰‹åŠ¨æ›´æ”¹å¤§ç‰ˆæœ¬å·ï¼ˆv1 â†’ v2ï¼‰
    # æ–°å¢éšæœºè£å‰ªå°ºå¯¸ï¼ˆç›®æ ‡å°ºå¯¸çš„ 80%ï¼‰
    "crop_size": (int(Config.IMAGE_SIZE[0] * 0.8), int(Config.IMAGE_SIZE[1] * 0.8)),
    # æ–°å¢å¼€å…³ï¼šæ˜¯å¦ä½¿ç”¨æ–°ç‰ˆçš„ä¸°å¯Œè”åˆæ•°æ®å¢å¼º
    "use_advanced_joint_augmentation": True
}

# æ ¹æ®å¼€å…³é€‰æ‹©è”åˆæ•°æ®å¢å¼ºæ–¹å¼
if params.get("use_advanced_joint_augmentation", False):
    joint_transform_fn = joint_transforms_albu
else:
    joint_transform_fn = joint_transforms

# ======================= 3. æ•°æ®é¢„å¤„ç† =======================
# -------------- è®­ç»ƒé›†é¢„å¤„ç†ï¼ˆå…ˆä½¿ç”¨è”åˆæ•°æ®å¢å¼ºï¼Œå†è¿›è¡Œ ToTensor ä¸å½’ä¸€åŒ–ï¼‰ --------------
train_transform_image = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

# ä½¿ç”¨ partial å°† crop_size å‚æ•°ä¼ å…¥ joint_transformsï¼Œä»…ç”¨äºè®­ç»ƒé›†
# train_joint_transform = partial(joint_transforms, crop_size=params["crop_size"])

# è®­ç»ƒé›† mask ä»…è¿›è¡Œ ToTensor å’Œæ ‡ç­¾è½¬æ¢ï¼ˆjoint_transforms å·²ç» Resize è¿‡ï¼‰
train_transform_mask = transforms.Compose([
    transforms.ToTensor(),
    lambda x: (x * 255).long().clamp(0, params["num_classes"] - 1)
])

# -------------- éªŒè¯é›†é¢„å¤„ç†ï¼ˆç›´æ¥ä½¿ç”¨ Resize, ToTensor ä¸å½’ä¸€åŒ–ï¼‰ --------------
val_transform_image = transforms.Compose([
    transforms.Resize(params["image_size"], interpolation=transforms.InterpolationMode.BILINEAR),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])
val_transform_mask = transforms.Compose([
    transforms.Resize(params["image_size"], interpolation=transforms.InterpolationMode.NEAREST),
    transforms.ToTensor(),
    lambda x: (x * 255).long().clamp(0, params["num_classes"] - 1)
])

# ======================= 4. åŠ è½½æ•°æ® =======================
image_dir = Config.DATA_PATHS['train_image_dir']
mask_dir = Config.DATA_PATHS['train_mask_dir']

# è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
image_files = sorted(os.listdir(image_dir))

# æŒ‰ 80% è®­ç»ƒï¼Œ20% éªŒè¯åˆ’åˆ†
train_files, val_files = train_test_split(image_files, test_size=0.2, random_state=42)

# train_dataset = ImageSegmentationDataset(image_dir, mask_dir, train_files, transform_image, transform_mask)
# val_dataset = ImageSegmentationDataset(image_dir, mask_dir, val_files, transform_image, transform_mask)


# åˆ›å»ºæ•°æ®é›†ï¼šè®­ç»ƒé›†å¯ç”¨è”åˆæ•°æ®å¢å¼ºï¼ˆç”± joint_transform_fn æ§åˆ¶ï¼‰ï¼ŒéªŒè¯é›†ä»…ä½¿ç”¨é¢„å¤„ç†è½¬æ¢
train_dataset = ImageSegmentationDataset(
    image_dir, mask_dir, train_files,
    transform_image=train_transform_image,
    transform_mask=train_transform_mask,
    joint_transform=joint_transform_fn,
    image_size=params["image_size"],
    # crop_size=params["crop_size"]
)
val_dataset = ImageSegmentationDataset(
    image_dir, mask_dir, val_files,
    transform_image=val_transform_image,
    transform_mask=val_transform_mask
    # éªŒè¯é›†ä¸ä½¿ç”¨è”åˆå¢å¼ºï¼Œç›´æ¥ç”± transform å®Œæˆ Resize
)

train_loader = DataLoader(train_dataset, batch_size=params["batch_size"], shuffle=True, num_workers=4)
val_loader = DataLoader(val_dataset, batch_size=params["batch_size"], shuffle=False, num_workers=4)

print(f"Training samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}")

# ======================= 5. åˆå§‹åŒ–æ¨¡å‹ =======================
model = NestedUNet(num_classes=params["num_classes"], input_channels=3).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.AdamW(model.parameters(), lr=params["learning_rate"], weight_decay=params["weight_decay"])
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode="min", factor=0.5, patience=5)

# ======================= 6. æ—©åœç­–ç•¥ =======================
class EarlyStopping:
    def __init__(self, patience=10, min_delta=0):
        self.patience = patience
        self.min_delta = min_delta
        self.best_loss = float("inf")
        self.counter = 0

    def __call__(self, val_loss):
        if val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
        else:
            self.counter += 1

        return self.counter >= self.patience

early_stopping = EarlyStopping(patience=params["patience"])


# ======================= ç‰ˆæœ¬ç®¡ç†å‡½æ•° =======================
def get_next_model_version(model_dir, base_version):
    """
    è‡ªåŠ¨å¢åŠ å°ç‰ˆæœ¬å·ï¼Œä¾‹å¦‚ï¼š
    - å½“å‰ç›®å½•ä¸‹ `v1.0` å­˜åœ¨ï¼Œåˆ™ç”Ÿæˆ `v1.1`
    - `v1.1` å­˜åœ¨ï¼Œåˆ™ç”Ÿæˆ `v1.2`
    """
    existing_versions = []

    if not os.path.exists(model_dir):
        os.makedirs(model_dir)

    for filename in os.listdir(model_dir):
        match = re.search(rf"{base_version}\.(\d+)", filename)
        if match:
            existing_versions.append(int(match.group(1)))

    if existing_versions:
        new_version = f"{base_version}.{max(existing_versions) + 1}"
    else:
        new_version = f"{base_version}.0"

    return new_version

def get_loss_optimizer_abbr(loss_fn, optimizer):
    """è·å–æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨çš„ç¼©å†™"""
    loss_abbr = {
        "CrossEntropyLoss": "CE",
        "MSELoss": "MSE",
        "DiceLoss": "Dice",
        "BCELoss": "BCE"
    }.get(loss_fn.__class__.__name__, "Loss")

    optim_abbr = {
        "SGD": "SGD",
        "Adam": "Adam",
        "AdamW": "AdamW",
        "RMSprop": "RMS"
    }.get(optimizer.__class__.__name__, "Opt")

    return loss_abbr, optim_abbr

#======================= ç”Ÿæˆæ—¶é—´æˆ³ï¼ˆç²¾ç¡®åˆ°å°æ—¶å’Œåˆ†é’Ÿï¼‰
timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M")

# è·å–è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„å›¾ç‰‡æ•°é‡
num_train_images = len(train_dataset)
num_val_images = len(val_dataset)

# è®¡ç®—æ–°çš„ç‰ˆæœ¬å·
model_dir = Config.DATA_PATHS['model_save_dir']
# model_dir = "./model_version_dir"
new_version = get_next_model_version(model_dir, params["model_version"])

# ç”Ÿæˆæ¨¡å‹æ–‡ä»¶å

# ç”Ÿæˆè¾“å…¥å°ºå¯¸å­—ç¬¦ä¸²ï¼ˆä¿®æ­£ {} é—®é¢˜ï¼‰
input_size_str = f"{params['image_size'][0]}x{params['image_size'][1]}"
# è·å–æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ç¼©å†™
loss_abbr, optim_abbr = get_loss_optimizer_abbr(criterion, optimizer)
# ç”Ÿæˆæ¨¡å‹æ–‡ä»¶å
model_filename = f"NestedUNet_{num_train_images}-{num_val_images}_{input_size_str}_{loss_abbr}_{optim_abbr}_{timestamp}_{new_version}.pth"
model_path = os.path.join(model_dir, model_filename)

# ======================= 7. è®­ç»ƒå‡½æ•° =======================
def train():
    best_val_loss = float("inf")

    for epoch in range(params["num_epochs"]):
        # è®­ç»ƒæ¨¡å¼
        model.train()
        epoch_loss = 0.0

        for images, masks in train_loader:
            images, masks = images.to(device), masks.to(device)

            optimizer.zero_grad()
            outputs = model(images)

            # ç¡®ä¿ `output` å½¢çŠ¶æ˜¯ `[batch_size, num_classes, H, W]`
            # print("Output Shape:", outputs.shape)

            # å¤„ç†æ·±åº¦ç›‘ç£
            if isinstance(outputs, list):
                loss = sum(criterion(out, masks) for out in outputs) / len(outputs)
            else:
                loss = criterion(outputs, masks)

            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()

        avg_train_loss = epoch_loss / len(train_loader)

        # ======================= 8. è®¡ç®—éªŒè¯é›†æŸå¤± =======================
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for images, masks in val_loader:
                images, masks = images.to(device), masks.to(device)
                outputs = model(images)

                if isinstance(outputs, list):
                    loss = sum(criterion(out, masks) for out in outputs) / len(outputs)
                else:
                    loss = criterion(outputs, masks)

                val_loss += loss.item()

        avg_val_loss = val_loss / len(val_loader)

        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(avg_val_loss)

        print(f"Epoch [{epoch + 1}/{params['num_epochs']}] | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}")

        # ======================= 9. æ—©åœæœºåˆ¶ =======================
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save(model.state_dict(), model_path)  # ä»…ä¿å­˜æœ€ä½³æ¨¡å‹
            print("âœ… Best model saved! " + model_filename)

        if early_stopping(avg_val_loss):
            print("âœ”âœ”âœ”âœ”âœ” Early stopping triggered!")
            break


# ======================= 10. è®­ç»ƒæ¨¡å‹ =======================
if __name__ == "__main__":
    train()